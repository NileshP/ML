
* Forward Propogation vs Back Propogation

	In neural networks, you forward propagate to get the output and compare it with the real value to get the error.
	Now,to minimize the error, you propagate backwards by finding the derivative of error with respect to each weight and the subtracting this value from the 	weight value.

	The basic learning that has to be done in neural networks is training neurons when to get activated.Each neuron should activate only for particular type of 		inputs and  not all inputs.So by propagating forward yu see how well your neural network is behaving and find the error.After you find out that your network 		has error, you backward propagate, and use a form of gradient descent to update new values of weights. then again you will forward propagate to see how well 	these weights are performing and then again will backward propagate to update the weights. This will go on until you reach some minima for error value